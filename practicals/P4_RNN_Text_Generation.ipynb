{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03b8f7fe",
   "metadata": {},
   "source": [
    "# P4: RNN/LSTM Text Generation\n",
    "\n",
    "**Objective:** Train a small LSTM to generate text on a sample dataset (IMDB or small text corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf09a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, preprocessing\n",
    "# This is a skeleton; students will prepare tokenized sequences and train an LSTM.\n",
    "print('Prepare dataset, tokenize, build embedding + LSTM, then sample generation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8353b381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 4: Character LSTM (PyTorch)\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "import random, math\n",
    "\n",
    "text = 'To be, or not to be, that is the question.'*200\n",
    "chars = sorted(list(set(text)))\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for ch,i in stoi.items()}\n",
    "data = torch.tensor([stoi[c] for c in text], dtype=torch.long)\n",
    "\n",
    "block=64\n",
    "def get_batch(sz=64):\n",
    "    ix = torch.randint(0, len(data)-block-1, (sz,))\n",
    "    X = torch.stack([data[i:i+block] for i in ix])\n",
    "    Y = torch.stack([data[i+1:i+block+1] for i in ix])\n",
    "    return X, Y\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab, hidden=128):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab, 64)\n",
    "        self.lstm = nn.LSTM(64, hidden, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden, vocab)\n",
    "    def forward(self, x, h=None):\n",
    "        x = self.emb(x)\n",
    "        out, h = self.lstm(x, h)\n",
    "        return self.fc(out), h\n",
    "\n",
    "vocab = len(chars)\n",
    "model = CharLSTM(vocab)\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "for step in range(200):\n",
    "    X,Y = get_batch()\n",
    "    logits,_ = model(X)\n",
    "    loss = crit(logits.view(-1, vocab), Y.view(-1))\n",
    "    opt.zero_grad(); loss.backward(); opt.step()\n",
    "    if step%50==0: print('step', step, 'loss', loss.item())\n",
    "\n",
    "def sample(prefix='To ' , length=120):\n",
    "    model.eval();\n",
    "    with torch.no_grad():\n",
    "        h=None; x=torch.tensor([[stoi[c] for c in prefix]], dtype=torch.long)\n",
    "        out=''+prefix\n",
    "        for t in range(length):\n",
    "            logits,h = model(x,h)\n",
    "            p = torch.softmax(logits[:,-1, :], dim=-1)\n",
    "            ix = torch.multinomial(p, num_samples=1)\n",
    "            ch = itos[ix.item()]\n",
    "            out+=ch\n",
    "            x = ix.view(1,1)\n",
    "    return out\n",
    "\n",
    "print(sample())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
