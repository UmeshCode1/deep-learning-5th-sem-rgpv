{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704fdd6e",
   "metadata": {},
   "source": [
    "# P1: MLP Implementation\n",
    "\n",
    "**Objective:** Implement a simple Multilayer Perceptron (MLP) using TensorFlow/Keras and report training/validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39a7874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal MLP example using Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28*28).astype('float32')/255.0\n",
    "x_test = x_test.reshape(-1, 28*28).astype('float32')/255.0\n",
    "model = models.Sequential([layers.Input(shape=(28*28,)), layers.Dense(128, activation='relu'), layers.Dropout(0.2), layers.Dense(10, activation='softmax')])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, validation_split=0.1, epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7cc19f",
   "metadata": {},
   "source": [
    "**Summary:** This notebook demonstrates a compact MLP pipeline. Extend by adding regularization, batch normalization, and experiments with optimizers."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
