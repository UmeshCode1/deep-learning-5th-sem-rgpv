# Unit IV â€” Recurrent Networks & Attention

## Topics
- RNNs, vanishing/exploding gradients
- LSTM and GRU architectures
- Attention mechanisms and Transformers (overview)
- Applications: Text generation, machine translation, video analytics

## Learning Outcomes
- Build sequence models using LSTM/GRU.
- Apply attention mechanisms for improved sequence modeling.

## Suggested Reading
- Chopra et al., Bahdanau et al. (attention), Transformer paper (Vaswani et al.)
